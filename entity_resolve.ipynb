{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.ncbi.nlm.nih.gov/research/pubtator3-api/publications/pmc_export/biocxml?pmcids=PMC9128899,PMC2927683\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"./dataset/paper.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response.text)\n",
    "    print(\"Data successfully saved to output.txt\")\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/5data_2/syash/MiniRAG/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from minirag.utils import xml_to_json\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import Levenshtein\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import concurrent.futures\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Constants\n",
    "WORKING_DIR = \"./input\"\n",
    "BATCH_SIZE_NODES = 500\n",
    "BATCH_SIZE_EDGES = 100\n",
    "\n",
    "# Neo4j connection credentials (consider using environment variables for security)\n",
    "NEO4J_URI = \"neo4j+s://5117636b.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"VcSOoWioxodkP0VXM-JVkYbn6SN39bCsfkJbwMeoXSc\"\n",
    "def load_graph_data(json_path):\n",
    "    \"\"\"Load JSON data from a file.\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data.get('nodes', []), data.get('edges', [])\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize entity names for consistency.\"\"\"\n",
    "    return name.lower().replace('-', ' ').replace('_', ' ')\n",
    "\n",
    "def compute_embeddings(entities):\n",
    "    model = SentenceTransformer('FremyCompany/BioLORD-2023') #FremyCompany/BioLORD-2023  pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\n",
    "    texts = []\n",
    "    for e in entities:\n",
    "        # Use only the normalized name, omitting the description\n",
    "        name = normalize_name(e['id'])\n",
    "        desc = e.get('description', '')\n",
    "        combined_text = f'{name} {desc}' if desc else name\n",
    "        texts.append(name)\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "def build_knn_graph(entities, embeddings, k=5, similarity_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Construct a k-NN graph using cosine similarity.\n",
    "    Uses NearestNeighbors to avoid computing the full similarity matrix.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    # Add nodes with their attributes\n",
    "    for entity in entities:\n",
    "        G.add_node(entity['id'], entity_type=entity['entity_type'])\n",
    "    \n",
    "    # Use cosine distance (note: similarity = 1 - distance)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "    for i, entity in enumerate(entities):\n",
    "        for j, dist in zip(indices[i][1:], distances[i][1:]):  # Skip self (first element)\n",
    "            score = 1 - dist\n",
    "            if score > similarity_threshold:\n",
    "                neighbor_id = entities[j]['id']\n",
    "                G.add_edge(entity['id'], neighbor_id, weight=score)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def find_merge_candidates(G):\n",
    "    \"\"\"Identify merge candidate groups based on connected components.\"\"\"\n",
    "    components = list(nx.connected_components(G))\n",
    "    return [list(component) for component in components if len(component) > 1]\n",
    "\n",
    "def are_entities_similar_lev(entity1, entity2):\n",
    "    \"\"\"Dynamically adjust the Levenshtein threshold based on entity length.\"\"\"\n",
    "    len_avg = (len(normalize_name(entity1)) + len(normalize_name(entity2))) / 2\n",
    "    threshold = 0.9 if len_avg < 10 else 0.8\n",
    "    sim = Levenshtein.ratio(normalize_name(entity1), normalize_name(entity2))\n",
    "    return sim >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "def log_decision(component, decision, explanation, log_file=\"llm_decisions_copy4.log\"):\n",
    "    \"\"\"Log LLM decisions for review and feedback.\"\"\"\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"\\n\\nComponent: {component}\\nDecision: {decision}\\nExplanation: {explanation}\\n{'-'*50}\\n\")\n",
    "        \n",
    "model_name = \"Horizon6957/DeepSeek-bio-vlarge-qna-cot\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# Set pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# Initialize generator pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # Explicitly use GPU 0\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def cached_llm_decision(component_tuple, entities_dict):\n",
    "    \"\"\"Cached version of LLM decision to avoid redundant calls.\"\"\"\n",
    "    component = list(component_tuple)\n",
    "    return llm_decision(component, entities_dict)\n",
    "\n",
    "\n",
    "def llm_decision(component, entities_dict):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert biologist specializing in entity resolution within biological networks. Your task is to determine whether the following entities represent the same biological concept and should therefore be merged. \n",
    "Consider the following criteria:\n",
    "1. No dublicate should be extract like entity full name and its abbreviation that should be merge, no seperate extraction for them.\n",
    "2. Do NOT group sub-types under broader categories. Instead, create separate nodes and establish relationships between them.\n",
    "3. Spelling variations or formatting differences like plural or singular forms should be merge\n",
    "Entities to evaluate:\n",
    "\"\"\"\n",
    "    for e in component:\n",
    "        prompt += f\"\\nEntity: {e}\\nDescription: {entities_dict[e].get('description', 'No description')}\\nType: {entities_dict[e].get('entity_type', 'Unknown type')}\\n\"\n",
    "    prompt += \"\\nShould these entities be merged? Please answer YES or NO and explain why.\"\n",
    "    \n",
    "    response = generator(prompt, max_new_tokens=512, truncation=True, do_sample=True)\n",
    "\n",
    "    \n",
    "    generated_txt = response[0][\"generated_text\"].strip().lower()\n",
    "\n",
    "    decision = \"yes\" if \"yes\" in generated_txt else \"no\"\n",
    "\n",
    "    return decision == \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root element: {http://graphml.graphdrawing.org/xmlns}graphml\n",
      "Root attributes: {'{http://www.w3.org/2001/XMLSchema-instance}schemaLocation': 'http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd'}\n",
      "Found 335 nodes and 291 edges\n",
      "JSON file created: ./input/graph_data.json\n",
      "Using LLM for merging candidates...\n",
      "LLM merge: ['DIFFUSE SYSTEMIC SCLEROSIS', 'SYSTEMIC SCLEROSIS', 'SYSTEMIC SCLEROSIS (SSC)']\n",
      "LLM merge: ['SYSTEMIC LUPUS ERYTHEMATOSUS (SLE)', 'SYSTEMIC LUPUS ERYTHEMATOSUS']\n",
      "LLM merge: ['T-CELL', 'T CELLS', 'T CELL']\n",
      "LLM merge: ['AUTOIMMUNE DISEASE', 'AUTOIMMUNE DISEASES', 'AUTOIMMUNITY', 'AUTOIMMUNE-LIKE DISEASES', 'AUTOIMMUNE AND INFLAMMATORY CONDITIONS']\n",
      "LLM merge: ['OX40L', 'OX40', 'SOLUBLE OX40L', 'OX40-OX40L', 'OXL40']\n",
      "LLM merge: ['RS944648', 'RS855648', 'RS2004640', 'RS844665', 'RS10912580', 'RS2205960', 'RS1234214', 'RS844648', 'RS1234315', 'RS844644', 'RS1234314', 'RS12039904', 'RS2795288']\n",
      "LLM merge: ['OX40 LIGAND', 'OX40/OX40 LIGAND']\n",
      "LLM merge: ['INFLAMMATORY', 'INFLAMMATION']\n",
      "LLM merge: ['ANTI-CENTROMERE ANTIBODIES', 'ANTI-CENTROMERE ANTIBODY', 'ANTI-CENTROMERE AUTOANTIBODIES', 'ANTI-CENTROMERE ANTIBODIES (ACA)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM merge: ['ANTI-TOPOISOMERASE I AUTOANTIBODIES', 'AUTOANTIBODIES TO TOPOISOMERASE I (ATA)', 'ANTI-TOPOISOMERASE I AUTOANTIBODY', 'ANTI-TOPOISOMERASE I ANTIBODIES', 'ANTI-TOPOISOMERASE I ANTIBODY']\n",
      "LLM merge: ['ANTI-RNA POLYMERASE III AUTOANTIBODIES', 'ANTI-RNA POLYMERASE III ANTIBODIES', 'ANTI-RNA POLYMERASE III ANTIBODIES (ARA)']\n",
      "LLM merge: ['SKIN INVOLVEMENT', 'CUTANEOUS INVOLVEMENT']\n",
      "LLM merge: ['B CELLS', 'B CELL']\n",
      "LLM merge: ['CD4+ T CELL', 'CD4 T CELL', 'CD4+ T CELLS']\n",
      "LLM merge: ['CD8+ T CELLS', 'CD8+ T CELL']\n",
      "LLM merge: ['PLASMACYTOID DENDRITIC CELLS', 'PLASMACYTOID DENDRITIC CELL']\n",
      "LLM merge: ['SNPS', 'SNP']\n",
      "LLM merge: ['TYPE I DIABETES', 'TYPE 1A DIABETES']\n",
      "LLM merge: ['ASTHMA', 'ALLERGIC ASTHMA']\n",
      "LLM merge: ['REGULATORY T-CELL', 'REGULATORY T-CELLS']\n",
      "LLM merge: ['MEMORY T-CELLS', 'MEMORY T CELLS']\n",
      "LLM merge: ['IL-4', 'IL-17']\n",
      "LLM merge: ['SKIN INFLAMMATION', 'INFLAMMATORY SKIN DISEASE', 'DERMATITIS', 'ECZEMA', 'INFLAMMATORY SKIN DISEASES']\n",
      "LLM merge: ['MAST CELLS', 'MAST CELL']\n",
      "LLM merge: ['ATOPIC DERMATITIS', 'ATOPIC DERMATITIS (AD)']\n",
      "LLM merge: ['ATOPIC COMORBIDITIES', 'ATOPIC DISEASES', 'ATOPIC DISEASE']\n",
      "LLM merge: ['ALEXA 488', 'ALEXA 647']\n",
      "LLM merge: ['CLA+ T CELLS', 'CLA T CELLS']\n",
      "LLM merge: ['CD8+CD45RO+CLA- T CELLS', 'CD8+CD45RO+ T CELL', 'CD4+CD45RO+OX40+ T CELLS', 'CD4+CD45RO+ T CELL', 'CD4+CD45RO+CLA+ T CELLS', 'CD8+CD45RO+CLA+ T CELLS', 'CD4+ CD45RO+ CLA+ T CELL', 'CD4+CD45RO+CLA- T CELLS']\n",
      "LLM merge: ['MONOCYTES', 'MONOCYTE']\n",
      "Comparing merged entity representatives with one another using Levenshtein...\n",
      "Levenshtein merge representatives: CD8+ T CELL -> CD4 T CELL\n",
      "Using Levenshtein distance for remaining entities...\n",
      "Levenshtein merge remaining: LIMITED SYSTEMIC SCLEROSIS -> SYSTEMIC SCLEROSIS\n",
      "Levenshtein merge remaining: TH CELLS -> T CELLS\n",
      "Levenshtein merge remaining: ANTICENTROMERE ANTIBODIES -> ANTI-CENTROMERE ANTIBODY\n",
      "Levenshtein merge remaining: ANTI-RNA POLYMERASE III -> ANTI-RNA POLYMERASE III ANTIBODIES\n",
      "Levenshtein merge remaining: CUTANOUS INVOLVEMENT -> CUTANEOUS INVOLVEMENT\n",
      "Levenshtein merge remaining: FOXP3+ REGULATORY T CELL -> REGULATORY T-CELLS\n",
      "Levenshtein merge remaining: TH2 MEMORY CELLS -> MEMORY T CELLS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def parallel_llm_decisions(components, entities_dict, max_workers=5):\n",
    "#     \"\"\"Parallelize LLM calls to speed up processing.\"\"\"\n",
    "#     decisions = {}\n",
    "#     with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         future_to_component = {\n",
    "#             executor.submit(cached_llm_decision, tuple(str(e) for e in comp), entities_dict): comp\n",
    "#             for comp in components\n",
    "#         }\n",
    "#         for future in concurrent.futures.as_completed(future_to_component):\n",
    "#             component = future_to_component[future]\n",
    "#             try:\n",
    "#                 decisions[tuple(str(e) for e in component)] = future.result()\n",
    "#             except Exception as exc:\n",
    "#                 print(f'Component {component} generated an exception: {exc}')\n",
    "#     return decisions\n",
    "\n",
    "def resolve_entities(entities, merge_candidates):\n",
    "    entities_dict = {e['id']: e for e in entities}\n",
    "    merged_entities = {}\n",
    "    remaining_entities = set(e['id'] for e in entities)\n",
    "    print(\"Using LLM for merging candidates...\")\n",
    "    \n",
    "    # Use LLM to decide on each merge candidate group\n",
    "    for group in merge_candidates:\n",
    "        # Only process groups whose entities havenâ€™t been merged yet\n",
    "        if not any(entity in merged_entities.get(rep, []) for rep in merged_entities for entity in group):\n",
    "            if llm_decision(group, entities_dict):\n",
    "                print(f\"LLM merge: {group}\")\n",
    "                main_entity = group[1]\n",
    "                merged_entities[main_entity] = group\n",
    "                remaining_entities -= set(group)\n",
    "    \n",
    "    print(\"Comparing merged entity representatives with one another using Levenshtein...\")\n",
    "    compared_pairs = set()\n",
    "    \n",
    "    # Compare merged entity representatives with one another using Levenshtein\n",
    "    representatives = list(merged_entities.keys())\n",
    "    for i in range(len(representatives)):\n",
    "        for j in range(i + 1, len(representatives)):\n",
    "            pair = tuple(sorted([representatives[i], representatives[j]]))\n",
    "            if pair not in compared_pairs:\n",
    "                compared_pairs.add(pair)\n",
    "                if are_entities_similar_lev(entities_dict[representatives[i]]['id'], entities_dict[representatives[j]]['id']):\n",
    "                    print(f\"Levenshtein merge representatives: {representatives[j]} -> {representatives[i]}\")\n",
    "                    merged_entities[representatives[i]].extend(merged_entities[representatives[j]])\n",
    "                    del merged_entities[representatives[j]]\n",
    "    \n",
    "    # Now, use Levenshtein distance for remaining entities\n",
    "    print(\"Using Levenshtein distance for remaining entities...\")\n",
    "    representatives = list(merged_entities.keys())  # Update list again\n",
    "    for rep in representatives:\n",
    "        for entity in list(remaining_entities):\n",
    "            if are_entities_similar_lev(entities_dict[rep]['id'], entities_dict[entity]['id']):\n",
    "                print(f\"Levenshtein merge remaining: {entity} -> {rep}\")\n",
    "                merged_entities[rep].append(entity)\n",
    "                remaining_entities.remove(entity)\n",
    "    \n",
    "    return merged_entities\n",
    "\n",
    "def create_entity_mapping(resolved_entities):\n",
    "    \"\"\"\n",
    "    Create a mapping for every entity in a merged group to its representative.\n",
    "    This mapping is then used to update both nodes and edges.\n",
    "    \"\"\"\n",
    "    entity_mapping = {}\n",
    "    for representative, group in resolved_entities.items():\n",
    "        for entity in group:\n",
    "            entity_mapping[entity] = representative\n",
    "    return entity_mapping\n",
    "def update_original_data(original_entities, entity_mapping):\n",
    "    \"\"\"\n",
    "    Replace all occurrences of merged entities with their representative entity.\n",
    "    \"\"\"\n",
    "    updated_entities = []\n",
    "    for entity in original_entities:\n",
    "        entity_id = entity['id']\n",
    "        if entity_id in entity_mapping:\n",
    "            entity['id'] = entity_mapping[entity_id]\n",
    "        updated_entities.append(entity)\n",
    "    return updated_entities\n",
    "def save_updated_data(updated_nodes, updated_edges, json_path):\n",
    "    \"\"\"\n",
    "    Save the resolved nodes and edges back to a JSON file.\n",
    "    \"\"\"\n",
    "    updated_data = {\n",
    "        \"nodes\": updated_nodes,\n",
    "        \"edges\": updated_edges\n",
    "    }\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(updated_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Updated data saved to {json_path}\")\n",
    "def convert_xml_to_json(xml_path, output_path):\n",
    "    \"\"\"\n",
    "    Converts an XML file to JSON and saves the output.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(xml_path):\n",
    "        print(f\"Error: File not found - {xml_path}\")\n",
    "        return None\n",
    "    json_data = xml_to_json(xml_path)\n",
    "    if json_data:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"JSON file created: {output_path}\")\n",
    "        return json_data\n",
    "    else:\n",
    "        print(\"Failed to create JSON data\")\n",
    "        return None\n",
    "def process_in_batches(tx, query, data, batch_size):\n",
    "    \"\"\"\n",
    "    Process data in batches and execute the given Neo4j query.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i : i + batch_size]\n",
    "        if \"nodes\" in query:\n",
    "            tx.run(query, {\"nodes\": batch})\n",
    "        else:\n",
    "            tx.run(query, {\"edges\": batch})\n",
    "xml_file = os.path.join(WORKING_DIR, \"graph_chunk_entity_relation.graphml\")\n",
    "json_file = os.path.join(WORKING_DIR, \"graph_data.json\")\n",
    "    \n",
    "# Convert XML to JSON\n",
    "json_data = convert_xml_to_json(xml_file, json_file)\n",
    "\n",
    "# 3. Load nodes and edges from JSON\n",
    "nodes = json_data.get(\"nodes\", [])\n",
    "edges = json_data.get(\"edges\", [])\n",
    "# 4. Compute embeddings and build the k-NN graph for de-duplication\n",
    "embeddings = compute_embeddings(nodes)\n",
    "G = build_knn_graph(nodes, embeddings)\n",
    "merge_candidates = find_merge_candidates(G)\n",
    "resolved_entities = resolve_entities(nodes, merge_candidates)\n",
    "entity_mapping = create_entity_mapping(resolved_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of nodes after de-duplication: 243\n",
      "Final number of edges after de-duplication: 231\n",
      "Updated data saved to ./input/graph_data.json\n"
     ]
    }
   ],
   "source": [
    "# 5. Update nodes and edges using the entity mapping\n",
    "updated_nodes = update_original_data(nodes, entity_mapping)\n",
    "updated_edges = [\n",
    "    {\n",
    "        \"source\": entity_mapping.get(edge[\"source\"], edge[\"source\"]),\n",
    "        \"target\": entity_mapping.get(edge[\"target\"], edge[\"target\"]),\n",
    "        \"weight\": edge.get(\"weight\", 1.0),\n",
    "        \"description\": edge.get(\"description\", \"\"),\n",
    "        \"keywords\": edge.get(\"keywords\", \"\"),\n",
    "        \"source_id\": edge.get(\"source_id\", \"\")\n",
    "    }\n",
    "    for edge in edges\n",
    "]\n",
    "\n",
    "# 6. Remove duplicate nodes based on their id\n",
    "unique_nodes = list({node['id']: node for node in updated_nodes}.values())\n",
    "    \n",
    "# 7. Remove duplicate edges and aggregate properties\n",
    "edge_dict = defaultdict(lambda: {\"weight\": 0, \"description\": \"\", \"keywords\": \"\", \"source_id\": \"\"})\n",
    "for edge in updated_edges:\n",
    "    key = (str(edge['source']), str(edge['target']))\n",
    "    edge_dict[key][\"weight\"] += edge.get(\"weight\", 1.0)\n",
    "    edge_dict[key][\"description\"] = edge.get(\"description\", \"\")\n",
    "    edge_dict[key][\"keywords\"] = edge.get(\"keywords\", \"\")\n",
    "    edge_dict[key][\"source_id\"] = edge.get(\"source_id\", \"\")\n",
    "unique_edges = [\n",
    "    {\"source\": source, \"target\": target, **properties}\n",
    "    for (source, target), properties in edge_dict.items()\n",
    "]\n",
    "print(f\"Final number of nodes after de-duplication: {len(unique_nodes)}\")\n",
    "print(f\"Final number of edges after de-duplication: {len(unique_edges)}\")\n",
    "save_updated_data(unique_nodes, unique_edges, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_nodes_query = \"\"\"\n",
    "UNWIND $nodes AS node\n",
    "MERGE (e:Entity {id: node.id})\n",
    "SET e.entity_type = node.entity_type,\n",
    "    e.description = node.description,\n",
    "    e.source_id = node.source_id,\n",
    "    e.displayName = node.id\n",
    "REMOVE e:Entity\n",
    "WITH e, node\n",
    "CALL apoc.create.addLabels(e, [node.id]) YIELD node AS labeledNode\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    "create_edges_query = \"\"\"\n",
    "UNWIND $edges AS edge\n",
    "MATCH (source {id: edge.source})\n",
    "MATCH (target {id: edge.target})\n",
    "WITH source, target, edge,\n",
    "        CASE\n",
    "        WHEN edge.keywords CONTAINS 'lead' THEN 'lead'\n",
    "        WHEN edge.keywords CONTAINS 'participate' THEN 'participate'\n",
    "        WHEN edge.keywords CONTAINS 'uses' THEN 'uses'\n",
    "        WHEN edge.keywords CONTAINS 'located' THEN 'located'\n",
    "        WHEN edge.keywords CONTAINS 'occurs' THEN 'occurs'\n",
    "        ELSE REPLACE(SPLIT(edge.keywords, ',')[0], '\\\"', '')\n",
    "        END AS relType\n",
    "CALL apoc.create.relationship(source, relType, {\n",
    "    weight: edge.weight,\n",
    "    description: edge.description,\n",
    "    keywords: edge.keywords,\n",
    "    source_id: edge.source_id\n",
    "}, target) YIELD rel\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    "set_displayname_and_labels_query = \"\"\"\n",
    "MATCH (n)\n",
    "SET n.displayName = n.id\n",
    "WITH n\n",
    "CALL apoc.create.setLabels(n, [n.entity_type]) YIELD node\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "            # Insert nodes in batches\n",
    "        session.execute_write(process_in_batches, create_nodes_query, unique_nodes, BATCH_SIZE_NODES)\n",
    "            # Insert edges in batches\n",
    "        session.execute_write(process_in_batches, create_edges_query, unique_edges, BATCH_SIZE_EDGES)\n",
    "            # Update displayName and labels\n",
    "        session.run(set_displayname_and_labels_query)\n",
    "except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "finally:\n",
    "    driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
