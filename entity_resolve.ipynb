{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to output.txt\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "\n",
    "# url = \"https://www.ncbi.nlm.nih.gov/research/pubtator3-api/publications/pmc_export/biocxml?pmcids=PMC9128899,PMC2927683\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     with open(\"./dataset/paper.txt\", \"w\", encoding=\"utf-6\") as file:\n",
    "#         file.write(response.text)\n",
    "#     print(\"Data successfully saved to output.txt\")\n",
    "# else:\n",
    "#     print(f\"Failed to fetch data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from minirag.utils import xml_to_json\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import Levenshtein\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import concurrent.futures\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Constants\n",
    "WORKING_DIR = \"./input\"\n",
    "BATCH_SIZE_NODES = 500\n",
    "BATCH_SIZE_EDGES = 100\n",
    "\n",
    "# Neo4j connection credentials (consider using environment variables for security)\n",
    "NEO4J_URI = \"neo4j+s://5117636b.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"VcSOoWioxodkP0VXM-JVkYbn6SN39bCsfkJbwMeoXSc\"\n",
    "def load_graph_data(json_path):\n",
    "    \"\"\"Load JSON data from a file.\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data.get('nodes', []), data.get('edges', [])\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize entity names for consistency.\"\"\"\n",
    "    return name.lower().replace('-', ' ').replace('_', ' ')\n",
    "\n",
    "def compute_embeddings(entities):\n",
    "    model = SentenceTransformer('FremyCompany/BioLORD-2023') #FremyCompany/BioLORD-2023  pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\n",
    "    texts = []\n",
    "    for e in entities:\n",
    "        # Use only the normalized name, omitting the description\n",
    "        name = normalize_name(e['id'])\n",
    "        desc = e.get('description', '')\n",
    "        combined_text = f'{name} {desc}' if desc else name\n",
    "        texts.append(name)\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "def build_knn_graph(entities, embeddings, k=5, similarity_threshold=0.87):\n",
    "\n",
    "    G = nx.Graph()\n",
    "    # Add nodes with their attributes\n",
    "    for entity in entities:\n",
    "        G.add_node(entity['id'], entity_type=entity['entity_type'])\n",
    "    \n",
    "    # Use cosine distance (note: similarity = 1 - distance)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "    for i, entity in enumerate(entities):\n",
    "        for j, dist in zip(indices[i][1:], distances[i][1:]):  # Skip self (first element)\n",
    "            score = 1 - dist\n",
    "            if score > similarity_threshold:\n",
    "                neighbor_id = entities[j]['id']\n",
    "                G.add_edge(entity['id'], neighbor_id, weight=score)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def find_merge_candidates(G):\n",
    "    \"\"\"Identify merge candidate groups based on connected components.\"\"\"\n",
    "    components = list(nx.connected_components(G))\n",
    "    return [list(component) for component in components if len(component) > 1]\n",
    "\n",
    "def are_entities_similar_lev(entity1, entity2):\n",
    "    \"\"\"Dynamically adjust the Levenshtein threshold based on entity length.\"\"\"\n",
    "    len_avg = (len(normalize_name(entity1)) + len(normalize_name(entity2))) / 2\n",
    "    threshold = 0.9 if len_avg < 10 else 0.8\n",
    "    sim = Levenshtein.ratio(normalize_name(entity1), normalize_name(entity2))\n",
    "    return sim >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/5data_2/syash/MiniRAG/.venv/lib/python3.12/site-packages/transformers/models/biogpt/tokenization_biogpt.py:104\u001b[39m, in \u001b[36mBioGptTokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, unk_token, bos_token, eos_token, sep_token, pad_token, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msacremoses\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sacremoses'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/5data_2/syash/MiniRAG/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2292\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2291\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2292\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2293\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/5data_2/syash/MiniRAG/.venv/lib/python3.12/site-packages/transformers/models/biogpt/tokenization_biogpt.py:106\u001b[39m, in \u001b[36mBioGptTokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, unk_token, bos_token, eos_token, sep_token, pad_token, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    107\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou need to install sacremoses to use BioGptTokenizer. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee https://pypi.org/project/sacremoses/ for installation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m     )\n\u001b[32m    111\u001b[39m \u001b[38;5;28mself\u001b[39m.lang = \u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: You need to install sacremoses to use BioGptTokenizer. See https://pypi.org/project/sacremoses/ for installation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m         f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mComponent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExplanation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplanation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mMedCat/MedCAT-PT-BioGPT-Large-v1-cosine_lr-checkpoint-260000\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model = AutoModelForCausalLM.from_pretrained(model_name)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Set pad token if missing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/5data_2/syash/MiniRAG/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:944\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    941\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    942\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    943\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m    947\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/5data_2/syash/MiniRAG/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2052\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2049\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2050\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2052\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2058\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2060\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/5data_2/syash/MiniRAG/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2293\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2291\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2292\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2293\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2294\u001b[39m     logger.info(\n\u001b[32m   2295\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2296\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2297\u001b[39m     )\n\u001b[32m   2298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/5data_2/syash/MiniRAG/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:87\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "def log_decision(component, decision, explanation, log_file=\"llm_decisions_copy.log\"):\n",
    "    \"\"\"Log LLM decisions for review and feedback.\"\"\"\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"\\n\\nComponent: {component}\\nDecision: {decision}\\nExplanation: {explanation}\\n{'-'*50}\\n\")\n",
    "        \n",
    "model_name = \"MedCat/MedCAT-PT-BioGPT-Large-v1-cosine_lr-checkpoint-260000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# Set pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# Initialize generator pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # Explicitly use GPU 0\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def cached_llm_decision(component_tuple, entities_dict):\n",
    "    \"\"\"Cached version of LLM decision to avoid redundant calls.\"\"\"\n",
    "    component = list(component_tuple)\n",
    "    return llm_decision(component, entities_dict)\n",
    "\n",
    "\n",
    "def llm_decision(component, entities_dict):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert biologist specializing in entity resolution within biological networks. Your task is to determine whether the following entities represent the same biological concept and should therefore be merged. \n",
    "Consider the following criteria:\n",
    "1. No dublicate should be extract like entity full name and its abbreviation that should be merge, no seperate extraction for them.\n",
    "2. Do NOT group sub-types under broader categories. Instead, create separate nodes and establish relationships between them.\n",
    "3. Spelling variations or formatting differences like plural or singular forms should be merge\n",
    "Entities to evaluate:\n",
    "\"\"\"\n",
    "    for e in component:\n",
    "        prompt += f\"\\nEntity: {e}\\nDescription: {entities_dict[e].get('description', 'No description')}\\nType: {entities_dict[e].get('entity_type', 'Unknown type')}\\n\"\n",
    "    prompt += \"\\nShould these entities be merged? Please answer YES or NO and explain why.\"\n",
    "    \n",
    "    response = generator(prompt, max_new_tokens=512, truncation=True, do_sample=True)\n",
    "\n",
    "    generated_txt = response[0][\"generated_text\"].strip().lower()\n",
    "\n",
    "    decision = \"yes\" if \"yes\" in generated_txt else \"no\"\n",
    "\n",
    "    explanation = generated_txt \n",
    "    log_decision(component, decision, explanation) \n",
    "\n",
    "    return decision == \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root element: {http://graphml.graphdrawing.org/xmlns}graphml\n",
      "Root attributes: {'{http://www.w3.org/2001/XMLSchema-instance}schemaLocation': 'http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd'}\n",
      "Found 335 nodes and 291 edges\n",
      "JSON file created: ./input/graph_data.json\n",
      "Using LLM for merging candidates...\n",
      "LLM merge: ['SYSTEMIC SCLEROSIS', 'SYSTEMIC SCLEROSIS (SSC)', 'PROGRESSIVE SYSTEMIC SCLEROSIS (SCLERODERMA)', 'DIFFUSE SYSTEMIC SCLEROSIS', 'PATIENTS WITH SYSTEMIC SCLEROSIS', 'LIMITED SYSTEMIC SCLEROSIS']\n",
      "LLM merge: ['SYSTEMIC LUPUS ERYTHEMATOSUS (SLE)', 'SYSTEMIC LUPUS ERYTHEMATOSUS']\n",
      "LLM merge: ['T-CELL', 'T CELL', 'T CELLS']\n",
      "LLM merge: ['AUTOIMMUNE AND INFLAMMATORY CONDITIONS', 'AUTOIMMUNITY', 'AUTOIMMUNE DISEASE', 'AUTOIMMUNE DISEASES', 'AUTOIMMUNE-LIKE DISEASES']\n",
      "LLM merge: ['OX40-OX40L', 'OX40/OX40L', 'OX40', 'OXL40', 'SOLUBLE OX40L', 'OX40L']\n",
      "LLM merge: ['RS844644', 'RS1234314', 'RS944648', 'RS855648', 'RS2205960', 'RS1234214', 'RS10912580', 'RS844665', 'RS1234315', 'RS2795288', 'RS2004640', 'RS844648', 'RS12039904']\n",
      "LLM merge: ['PATIENTS', 'PATIENT']\n",
      "LLM merge: ['OX40 LIGAND', 'OX40/OX40 LIGAND']\n",
      "LLM merge: ['INFLAMMATION', 'INFLAMMATORY', 'INFLAMMATORY REACTION']\n",
      "LLM merge: ['ANTI-CENTROMERE ANTIBODIES', 'ANTI-CENTROMERE ANTIBODY', 'ANTI-CENTROMERE AUTOANTIBODIES', 'ANTI-CENTROMERE ANTIBODIES (ACA)']\n",
      "LLM merge: ['ANTI-TOPOISOMERASE I ANTIBODY', 'AUTOANTIBODIES TO TOPOISOMERASE I (ATA)', 'ANTI-TOPOISOMERASE I ANTIBODIES', 'ANTI-TOPOISOMERASE I AUTOANTIBODIES', 'ANTI-TOPOISOMERASE I AUTOANTIBODY']\n",
      "LLM merge: ['ANTI-RNA POLYMERASE III ANTIBODIES (ARA)', 'ANTI-RNA POLYMERASE III AUTOANTIBODIES', 'ANTI-RNA POLYMERASE III ANTIBODIES']\n",
      "LLM merge: ['HEALTHY CONTROL (HC)', 'HEALTHY CONTROLS']\n",
      "LLM merge: ['SKIN INVOLVEMENT', 'CUTANOUS INVOLVEMENT', 'CUTANEOUS INVOLVEMENT']\n",
      "LLM merge: ['B CELL', 'B CELLS']\n",
      "LLM merge: ['CD4+ T CELL', 'CD4 T CELL', 'CD4+ T CELLS']\n",
      "LLM merge: ['CD8+ T CELL', 'CD8+ T CELLS']\n",
      "LLM merge: ['INTERLEUKIN 17', 'IL-4', 'IL-17']\n",
      "LLM merge: ['PLASMACYTOID DENDRITIC CELL', 'PLASMACYTOID DENDRITIC CELLS']\n",
      "LLM merge: ['SNPS', 'SNP']\n",
      "LLM merge: ['TYPE I DIABETES', 'TYPE 1A DIABETES']\n",
      "LLM merge: ['ALLERGIC ASTHMA', 'ASTHMA']\n",
      "LLM merge: ['REGULATORY T-CELL', 'REGULATORY T-CELLS']\n",
      "LLM merge: ['MEMORY T CELLS', 'MEMORY T-CELLS']\n",
      "LLM merge: ['SOX40L', 'SOX40']\n",
      "LLM merge: ['INFLAMMATORY SKIN DISEASES', 'DERMATITIS', 'INFLAMMATORY SKIN DISEASE', 'SKIN INFLAMMATION', 'ECZEMA']\n",
      "LLM merge: ['MAST CELLS', 'MAST CELL']\n",
      "LLM merge: ['ATOPIC DERMATITIS', 'ATOPIC DERMATITIS (AD)']\n",
      "LLM merge: ['IL-25', 'INTERLEUKIN-25']\n",
      "LLM merge: ['ALLERGIC REACTION', 'HYPERSENSITIVITY']\n",
      "LLM merge: ['ATOPIC DISEASE', 'ATOPIC DISEASES', 'ATOPIC COMORBIDITIES']\n",
      "LLM merge: ['ALEXA 647', 'ALEXA 488']\n",
      "LLM merge: ['CLA T CELLS', 'CLA+ T CELLS']\n",
      "LLM merge: ['CD4+CD45RO+ T CELL', 'CD4+CD45RO+OX40+ T CELLS', 'CD4+ CD45RO+ CLA+ T CELL', 'CD4+CD45RO+CLA- T CELLS', 'CD8+CD45RO+CLA- T CELLS', 'CD4+CD45RO+CLA+ T CELLS', 'CD8+CD45RO+ T CELL', 'CD8+CD45RO+CLA+ T CELLS']\n",
      "LLM merge: ['MONOCYTES', 'MONOCYTE']\n",
      "Comparing merged entity representatives with one another using Levenshtein...\n",
      "Levenshtein merge representatives: CD8+ T CELL -> CD4+ T CELL\n",
      "Using Levenshtein distance for remaining entities...\n",
      "Levenshtein merge remaining: OX40-OX40L AXIS -> OX40-OX40L\n",
      "Levenshtein merge remaining: ANTICENTROMERE ANTIBODIES -> ANTI-CENTROMERE ANTIBODIES\n",
      "Levenshtein merge remaining: ANTI-TOPOISOMERASE I -> ANTI-TOPOISOMERASE I ANTIBODY\n",
      "Levenshtein merge remaining: FOXP3+ REGULATORY T CELL -> REGULATORY T-CELL\n",
      "Levenshtein merge remaining: TH2 MEMORY CELLS -> MEMORY T CELLS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def parallel_llm_decisions(components, entities_dict, max_workers=5):\n",
    "#     \"\"\"Parallelize LLM calls to speed up processing.\"\"\"\n",
    "#     decisions = {}\n",
    "#     with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         future_to_component = {\n",
    "#             executor.submit(cached_llm_decision, tuple(str(e) for e in comp), entities_dict): comp\n",
    "#             for comp in components\n",
    "#         }\n",
    "#         for future in concurrent.futures.as_completed(future_to_component):\n",
    "#             component = future_to_component[future]\n",
    "#             try:\n",
    "#                 decisions[tuple(str(e) for e in component)] = future.result()\n",
    "#             except Exception as exc:\n",
    "#                 print(f'Component {component} generated an exception: {exc}')\n",
    "#     return decisions\n",
    "\n",
    "def resolve_entities(entities, merge_candidates):\n",
    "    entities_dict = {e['id']: e for e in entities}\n",
    "    merged_entities = {}\n",
    "    remaining_entities = set(e['id'] for e in entities)\n",
    "    print(\"Using LLM for merging candidates...\")\n",
    "    \n",
    "    # Use LLM to decide on each merge candidate group\n",
    "    for group in merge_candidates:\n",
    "        if not any(entity in merged_entities.get(rep, []) for rep in merged_entities for entity in group):\n",
    "            if llm_decision(group, entities_dict):\n",
    "                print(f\"LLM merge: {group}\")\n",
    "                main_entity = group[0]\n",
    "                merged_entities[main_entity] = group\n",
    "                remaining_entities -= set(group)\n",
    "    \n",
    "    print(\"Comparing merged entity representatives with one another using Levenshtein...\")\n",
    "    compared_pairs = set()\n",
    "    representatives = list(merged_entities.keys())\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(representatives):\n",
    "        j = i + 1\n",
    "        while j < len(representatives):\n",
    "            pair = tuple(sorted([representatives[i], representatives[j]]))\n",
    "            if pair not in compared_pairs:\n",
    "                compared_pairs.add(pair)\n",
    "                if are_entities_similar_lev(entities_dict[representatives[i]]['id'], entities_dict[representatives[j]]['id']):\n",
    "                    print(f\"Levenshtein merge representatives: {representatives[j]} -> {representatives[i]}\")\n",
    "                    if representatives[j] in merged_entities: #check if key exist before merge\n",
    "                        merged_entities[representatives[i]].extend(merged_entities[representatives[j]])\n",
    "                        del merged_entities[representatives[j]]\n",
    "                        representatives.pop(j) #adjust index, and representatives list after del.\n",
    "                        continue #skip j +=1 because list was modified\n",
    "            j+=1\n",
    "        i+=1\n",
    "\n",
    "    print(\"Using Levenshtein distance for remaining entities...\")\n",
    "    representatives = list(merged_entities.keys())\n",
    "    for rep in representatives:\n",
    "        for entity in list(remaining_entities):\n",
    "            if are_entities_similar_lev(entities_dict[rep]['id'], entities_dict[entity]['id']):\n",
    "                print(f\"Levenshtein merge remaining: {entity} -> {rep}\")\n",
    "                merged_entities[rep].append(entity)\n",
    "                remaining_entities.remove(entity)\n",
    "    \n",
    "    return merged_entities\n",
    "\n",
    "def create_entity_mapping(resolved_entities):\n",
    "    \"\"\"\n",
    "    Create a mapping for every entity in a merged group to its representative.\n",
    "    This mapping is then used to update both nodes and edges.\n",
    "    \"\"\"\n",
    "    entity_mapping = {}\n",
    "    for representative, group in resolved_entities.items():\n",
    "        for entity in group:\n",
    "            entity_mapping[entity] = representative\n",
    "    return entity_mapping\n",
    "def update_original_data(original_entities, entity_mapping):\n",
    "    \"\"\"\n",
    "    Replace all occurrences of merged entities with their representative entity.\n",
    "    \"\"\"\n",
    "    updated_entities = []\n",
    "    for entity in original_entities:\n",
    "        entity_id = entity['id']\n",
    "        if entity_id in entity_mapping:\n",
    "            entity['id'] = entity_mapping[entity_id]\n",
    "        updated_entities.append(entity)\n",
    "    return updated_entities\n",
    "def save_updated_data(updated_nodes, updated_edges, json_path):\n",
    "    \"\"\"\n",
    "    Save the resolved nodes and edges back to a JSON file.\n",
    "    \"\"\"\n",
    "    updated_data = {\n",
    "        \"nodes\": updated_nodes,\n",
    "        \"edges\": updated_edges\n",
    "    }\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(updated_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Updated data saved to {json_path}\")\n",
    "def convert_xml_to_json(xml_path, output_path):\n",
    "    \"\"\"\n",
    "    Converts an XML file to JSON and saves the output.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(xml_path):\n",
    "        print(f\"Error: File not found - {xml_path}\")\n",
    "        return None\n",
    "    json_data = xml_to_json(xml_path)\n",
    "    if json_data:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"JSON file created: {output_path}\")\n",
    "        return json_data\n",
    "    else:\n",
    "        print(\"Failed to create JSON data\")\n",
    "        return None\n",
    "def process_in_batches(tx, query, data, batch_size):\n",
    "    \"\"\"\n",
    "    Process data in batches and execute the given Neo4j query.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i : i + batch_size]\n",
    "        if \"nodes\" in query:\n",
    "            tx.run(query, {\"nodes\": batch})\n",
    "        else:\n",
    "            tx.run(query, {\"edges\": batch})\n",
    "xml_file = os.path.join(WORKING_DIR, \"graph_chunk_entity_relation.graphml\")\n",
    "json_file = os.path.join(WORKING_DIR, \"graph_data.json\")\n",
    "    \n",
    "# Convert XML to JSON\n",
    "json_data = convert_xml_to_json(xml_file, json_file)\n",
    "\n",
    "# 3. Load nodes and edges from JSON\n",
    "nodes = json_data.get(\"nodes\", [])\n",
    "edges = json_data.get(\"edges\", [])\n",
    "# 4. Compute embeddings and build the k-NN graph for de-duplication\n",
    "embeddings = compute_embeddings(nodes)\n",
    "G = build_knn_graph(nodes, embeddings)\n",
    "merge_candidates = find_merge_candidates(G)\n",
    "resolved_entities = resolve_entities(nodes, merge_candidates)\n",
    "entity_mapping = create_entity_mapping(resolved_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of nodes after de-duplication: 243\n",
      "Final number of edges after de-duplication: 231\n",
      "Updated data saved to ./input/graph_data.json\n"
     ]
    }
   ],
   "source": [
    "# 5. Update nodes and edges using the entity mapping\n",
    "updated_nodes = update_original_data(nodes, entity_mapping)\n",
    "updated_edges = [\n",
    "    {\n",
    "        \"source\": entity_mapping.get(edge[\"source\"], edge[\"source\"]),\n",
    "        \"target\": entity_mapping.get(edge[\"target\"], edge[\"target\"]),\n",
    "        \"weight\": edge.get(\"weight\", 1.0),\n",
    "        \"description\": edge.get(\"description\", \"\"),\n",
    "        \"keywords\": edge.get(\"keywords\", \"\"),\n",
    "        \"source_id\": edge.get(\"source_id\", \"\")\n",
    "    }\n",
    "    for edge in edges\n",
    "]\n",
    "\n",
    "# 6. Remove duplicate nodes based on their id\n",
    "unique_nodes = list({node['id']: node for node in updated_nodes}.values())\n",
    "    \n",
    "# 7. Remove duplicate edges and aggregate properties\n",
    "edge_dict = defaultdict(lambda: {\"weight\": 0, \"description\": \"\", \"keywords\": \"\", \"source_id\": \"\"})\n",
    "for edge in updated_edges:\n",
    "    key = (str(edge['source']), str(edge['target']))\n",
    "    edge_dict[key][\"weight\"] += edge.get(\"weight\", 1.0)\n",
    "    edge_dict[key][\"description\"] = edge.get(\"description\", \"\")\n",
    "    edge_dict[key][\"keywords\"] = edge.get(\"keywords\", \"\")\n",
    "    edge_dict[key][\"source_id\"] = edge.get(\"source_id\", \"\")\n",
    "unique_edges = [\n",
    "    {\"source\": source, \"target\": target, **properties}\n",
    "    for (source, target), properties in edge_dict.items()\n",
    "]\n",
    "print(f\"Final number of nodes after de-duplication: {len(unique_nodes)}\")\n",
    "print(f\"Final number of edges after de-duplication: {len(unique_edges)}\")\n",
    "save_updated_data(unique_nodes, unique_edges, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_nodes_query = \"\"\"\n",
    "UNWIND $nodes AS node\n",
    "MERGE (e:Entity {id: node.id})\n",
    "SET e.entity_type = node.entity_type,\n",
    "    e.description = node.description,\n",
    "    e.source_id = node.source_id,\n",
    "    e.displayName = node.id\n",
    "REMOVE e:Entity\n",
    "WITH e, node\n",
    "CALL apoc.create.addLabels(e, [node.id]) YIELD node AS labeledNode\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    "create_edges_query = \"\"\"\n",
    "UNWIND $edges AS edge\n",
    "MATCH (source {id: edge.source})\n",
    "MATCH (target {id: edge.target})\n",
    "WITH source, target, edge,\n",
    "        CASE\n",
    "        WHEN edge.keywords CONTAINS 'lead' THEN 'lead'\n",
    "        WHEN edge.keywords CONTAINS 'participate' THEN 'participate'\n",
    "        WHEN edge.keywords CONTAINS 'uses' THEN 'uses'\n",
    "        WHEN edge.keywords CONTAINS 'located' THEN 'located'\n",
    "        WHEN edge.keywords CONTAINS 'occurs' THEN 'occurs'\n",
    "        ELSE REPLACE(SPLIT(edge.keywords, ',')[0], '\\\"', '')\n",
    "        END AS relType\n",
    "CALL apoc.create.relationship(source, relType, {\n",
    "    weight: edge.weight,\n",
    "    description: edge.description,\n",
    "    keywords: edge.keywords,\n",
    "    source_id: edge.source_id\n",
    "}, target) YIELD rel\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    "set_displayname_and_labels_query = \"\"\"\n",
    "MATCH (n)\n",
    "SET n.displayName = n.id\n",
    "WITH n\n",
    "CALL apoc.create.setLabels(n, [n.entity_type]) YIELD node\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "            # Insert nodes in batches\n",
    "        session.execute_write(process_in_batches, create_nodes_query, unique_nodes, BATCH_SIZE_NODES)\n",
    "            # Insert edges in batches\n",
    "        session.execute_write(process_in_batches, create_edges_query, unique_edges, BATCH_SIZE_EDGES)\n",
    "            # Update displayName and labels\n",
    "        session.run(set_displayname_and_labels_query)\n",
    "except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "finally:\n",
    "    driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
